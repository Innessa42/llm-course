# Библиотеки для RAG с использованием Google Gemini
import os
import numpy as np
from dotenv import load_dotenv
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import CharacterTextSplitter
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI

# Загрузка переменных окружения
load_dotenv()  # загружаем ключи API из файла .env

# Настройка Google Gemini API
os.environ["GOOGLE_API_KEY"] = os.getenv("GEMINI_API_KEY")

# ---------------- ПОДГОТОВКА ДАННЫХ ----------------

# Пример документов о разных технологиях ИИ (можно заменить на любые другие документы)
# Эти тексты будут использованы для создания базы знаний, на основе которой система сможет отвечать на запросы.
documents = [
    """Генеративно-состязательные сети (GAN) - это архитектура нейронных сетей, 
    предложенная Яном Гудфеллоу в 2014 году. GAN состоит из двух нейронных сетей: 
    генератора и дискриминатора. Генератор создает синтетические данные, а дискриминатор 
    пытается отличить их от реальных данных. Эти две сети обучаются в процессе соревнования, 
    что приводит к улучшению качества генерируемых данных. GAN используются для генерации 
    изображений, текста, музыки и других типов данных.""",

    """Трансформеры - это архитектура нейронных сетей, представленная в статье 'Attention 
    is All You Need' в 2017 году. Ключевым компонентом трансформеров является механизм 
    внимания (attention), который позволяет модели уделять различное внимание разным 
    частям входных данных. Трансформеры произвели революцию в обработке естественного 
    языка и используются в таких моделях, как BERT, GPT, T5 и других. Они хорошо 
    масштабируются и эффективно обрабатывают последовательности.""",

    """Глубокое обучение с подкреплением (Deep Reinforcement Learning) объединяет 
    глубокие нейронные сети с обучением с подкреплением. В этом подходе агент учится 
    максимизировать награду через взаимодействие с окружающей средой. Примеры успешного 
    применения включают AlphaGo от DeepMind, который победил чемпиона мира по игре в го, 
    и системы управления роботами в сложных средах. Ключевыми алгоритмами являются 
    DQN, A3C, PPO и SAC.""",

    """Нейронные сети с остаточными связями (ResNet) были представлены Microsoft Research 
    в 2015 году. Главная идея ResNet - использование "пропускных соединений", которые 
    позволяют сигналам обходить некоторые слои сети. Это помогает решить проблему 
    затухающих градиентов при обучении очень глубоких нейронных сетей. ResNet позволил 
    создавать сети глубиной более 100 слоев и значительно улучшил точность в задачах 
    компьютерного зрения."""
]

# Разбиваем документы на фрагменты (chunks)
# Создаем объект text_splitter для разбиения текста на части.
# Параметр chunk_size=200 указывает, что каждый фрагмент будет содержать до 200 символов.
# Параметр chunk_overlap=0 означает, что фрагменты не будут пересекаться.
text_splitter = CharacterTextSplitter(chunk_size=200, chunk_overlap=0)
chunks = []
# Проходим по каждому документу и разбиваем его на фрагменты, добавляя их в список chunks.
for doc in documents:
    chunks.extend(text_splitter.split_text(doc))

# Выводим количество полученных фрагментов, чтобы понять, насколько документ разбился на части.
print(f"Документы разбиты на {len(chunks)} фрагментов")

# ---------------- СОЗДАНИЕ ВЕКТОРНОЙ БАЗЫ ДАННЫХ ----------------

# Создаем эмбеддинги с помощью Google Gemini и сохраняем их в векторную базу данных
embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
# С помощью FAISS создаем векторную базу данных из полученных фрагментов.
vector_store = FAISS.from_texts(chunks, embeddings)

print("Векторная база данных создана!")


# ---------------- ФУНКЦИЯ ДЛЯ ЗАПРОСОВ К RAG ----------------


def ask_rag(query, vector_store, num_results=2):
    """
    Функция для запросов к системе RAG

    Аргументы:
        query (str): Запрос пользователя
        vector_store: Векторная база данных
        num_results (int): Количество возвращаемых документов

    Возвращает:
        str: Ответ от LLM, дополненный релевантным контекстом
    """
    # 1. Поиск релевантных фрагментов (retrieval)
    print("\nПоиск релевантных фрагментов для запроса...")

    # Функция similarity_search ищет в векторной базе наиболее похожие фрагменты по данному запросу.
    retrieved_docs = vector_store.similarity_search(query, k=num_results)

    # Выводим найденные фрагменты для демонстрации
    print("\nНайденные релевантные фрагменты:")
    relevant_context = ""
    # Перебираем найденные документы, выводим начало каждого и объединяем их в общий контекст.
    for i, doc in enumerate(retrieved_docs):
        print(f"Фрагмент {i + 1}: {doc.page_content[:100]}...")
        relevant_context += doc.page_content + "\n\n"

    # 2. Генерация ответа с использованием найденных фрагментов (generation)
    print("\nГенерация ответа с использованием LLM и найденных фрагментов...")

    # Создаем шаблон запроса (prompt), который содержит инструкции для языковой модели.
    # В шаблоне указано, что нужно использовать предоставленный контекст для ответа.
    prompt_template = """
    Ты - полезный ИИ-ассистент. Используй следующий контекст, чтобы ответить на вопрос пользователя.
    Если ответа нет в контексте, просто скажи, что у тебя нет этой информации.

    Контекст:
    {context}

    Вопрос пользователя: {query}

    Твой ответ:
    """

    # Инициализируем PromptTemplate с заданным шаблоном и переменными, которые будут заменены на конкретные значения.
    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "query"]
    )

    # Создаем объект языковой модели, используя Google Gemini.
    llm = ChatGoogleGenerativeAI(
        model="gemini-2.0-flash",
        temperature=0,
        convert_system_message_to_human=True,
    )
    # Объединяем языковую модель и шаблон запроса в цепочку (LLMChain),
    # что позволяет автоматически подставлять переменные в шаблон и получать ответ.
    chain = LLMChain(llm=llm, prompt=prompt)

    # Генерируем ответ
    response = chain.run(context=relevant_context, query=query)

    return response


# ---------------- ПРИМЕР ИСПОЛЬЗОВАНИЯ ----------------

def demo_rag():
    """Демонстрация работы RAG системы"""
    print("\n" + "=" * 50)
    print("ДЕМОНСТРАЦИЯ РАБОТЫ RAG с Google Gemini")
    print("=" * 50)

    # Пример запросов
    queries = [
        "Что такое трансформеры и где они используются?",
        "Расскажи про GAN и для чего они нужны",
        "Что такое автоэнкодеры?"  # Информации об этом нет в нашей базе данных
    ]

    # Для каждого запроса выполняем поиск релевантного контекста и генерацию ответа.
    for i, query in enumerate(queries):
        print(f"\n\nЗАПРОС {i + 1}: {query}")
        response = ask_rag(query, vector_store)
        print("\nОТВЕТ:")
        print(response)
        print("\n" + "-" * 50)


# Запускаем демонстрацию
if __name__ == "__main__":
    demo_rag()
